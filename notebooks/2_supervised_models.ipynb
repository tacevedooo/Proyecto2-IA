{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4340e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librer√≠as necesarias\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# diccionario para guardar todas las m√©tricas\n",
    "metricas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86804070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight_kg</th>\n",
       "      <th>Height_m</th>\n",
       "      <th>Max_BPM</th>\n",
       "      <th>Avg_BPM</th>\n",
       "      <th>Resting_BPM</th>\n",
       "      <th>Session_Duration_hours</th>\n",
       "      <th>Calories_Burned</th>\n",
       "      <th>Workout_Type</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.14</td>\n",
       "      <td>101.05</td>\n",
       "      <td>1.95</td>\n",
       "      <td>171.17</td>\n",
       "      <td>130.81</td>\n",
       "      <td>68.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>959.43</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.17</td>\n",
       "      <td>41.63</td>\n",
       "      <td>1.78</td>\n",
       "      <td>167.33</td>\n",
       "      <td>158.46</td>\n",
       "      <td>63.95</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1424.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.07</td>\n",
       "      <td>63.81</td>\n",
       "      <td>1.78</td>\n",
       "      <td>187.86</td>\n",
       "      <td>137.11</td>\n",
       "      <td>60.93</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1766.64</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.30</td>\n",
       "      <td>59.77</td>\n",
       "      <td>1.78</td>\n",
       "      <td>183.83</td>\n",
       "      <td>120.32</td>\n",
       "      <td>60.01</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1028.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.99</td>\n",
       "      <td>57.60</td>\n",
       "      <td>1.56</td>\n",
       "      <td>166.25</td>\n",
       "      <td>151.82</td>\n",
       "      <td>67.97</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1295.80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Weight_kg  Height_m  Max_BPM  Avg_BPM  Resting_BPM  \\\n",
       "0  21.14     101.05      1.95   171.17   130.81        68.96   \n",
       "1  44.17      41.63      1.78   167.33   158.46        63.95   \n",
       "2  20.07      63.81      1.78   187.86   137.11        60.93   \n",
       "3  36.30      59.77      1.78   183.83   120.32        60.01   \n",
       "4  51.99      57.60      1.56   166.25   151.82        67.97   \n",
       "\n",
       "   Session_Duration_hours  Calories_Burned  Workout_Type  Gender_Female  \\\n",
       "0                    0.97           959.43             2            0.0   \n",
       "1                    1.48          1424.35             0            0.0   \n",
       "2                    1.70          1766.64             0            1.0   \n",
       "3                    0.85          1028.50             1            1.0   \n",
       "4                    1.66          1295.80             3            0.0   \n",
       "\n",
       "   Gender_Male  \n",
       "0          1.0  \n",
       "1          1.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Carga del Conjunto de Datos ---\n",
    "data = pd.read_csv(\"../data/subset/clean_subset_lifestyledata_rows5200_seed5200.csv\")\n",
    "\n",
    "# --- 2. Codificaci√≥n de Etiquetas (Label Encoding) ---\n",
    "label_encoder = LabelEncoder()\n",
    "# Se transforma la variable objetivo 'Workout_Type' a valores num√©ricos.\n",
    "data['Workout_Type'] = label_encoder.fit_transform(data['Workout_Type'])\n",
    "\n",
    "# --- 3. Codificaci√≥n One-Hot (One-Hot Encoding) ---\n",
    "# Se define la lista de columnas categ√≥ricas nominales a transformar.\n",
    "nominal_cols = ['Gender']\n",
    "# sparse_output=False: Devuelve una matriz densa (array de NumPy) en lugar de una dispersa.\n",
    "# handle_unknown='ignore': Si aparece una categor√≠a no vista durante la transformaci√≥n, la ignora.\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "# Esto crea nuevas columnas binarias para cada categor√≠a.\n",
    "encoded = ohe.fit_transform(data[nominal_cols])\n",
    "# Se convierte la matriz resultante en un DataFrame con nombres de columna apropiados.\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(nominal_cols))\n",
    "\n",
    "# --- 4. Combinaci√≥n de los Datos Procesados ---\n",
    "# Se elimina la columna original 'Gender' del DataFrame principal.\n",
    "# reset_index(drop=True) asegura que los √≠ndices se alineen correctamente para la concatenaci√≥n.\n",
    "data = data.drop(columns=nominal_cols).reset_index(drop=True)\n",
    "encoded_df = encoded_df.reset_index(drop=True)\n",
    "\n",
    "# Se concatenan el DataFrame original y el nuevo DataFrame con las columnas codificadas.\n",
    "# axis=1 indica que la uni√≥n se realiza por columnas.\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "# --- 5. Visualizaci√≥n ---\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d8ea8",
   "metadata": {},
   "source": [
    "# √Årbol de Decisiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928020a",
   "metadata": {},
   "source": [
    "#### 1. √Årbol de Decisiones - CC:SI - ED:NO - Outliers:NO - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b46c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== üß† CASO DE PRUEBA 1 (random_state=111) ===========\n",
      "\n",
      "Modelo: Gini\n",
      "Accuracy: 0.7327, Precision: 0.7412, Recall: 0.7327, F1-Score: 0.7350\n",
      "\n",
      "Modelo: Entrop√≠a\n",
      "Accuracy: 0.7779, Precision: 0.7923, Recall: 0.7779, F1-Score: 0.7804\n",
      "\n",
      "Modelo: Entrop√≠a Podado\n",
      "Accuracy: 0.7779, Precision: 0.7923, Recall: 0.7779, F1-Score: 0.7804\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 2 (random_state=222) ===========\n",
      "\n",
      "Modelo: Gini\n",
      "Accuracy: 0.7452, Precision: 0.7431, Recall: 0.7452, F1-Score: 0.7438\n",
      "\n",
      "Modelo: Entrop√≠a\n",
      "Accuracy: 0.7875, Precision: 0.7968, Recall: 0.7875, F1-Score: 0.7892\n",
      "\n",
      "Modelo: Entrop√≠a Podado\n",
      "Accuracy: 0.7875, Precision: 0.7968, Recall: 0.7875, F1-Score: 0.7892\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 3 (random_state=333) ===========\n",
      "\n",
      "Modelo: Gini\n",
      "Accuracy: 0.7587, Precision: 0.7538, Recall: 0.7587, F1-Score: 0.7559\n",
      "\n",
      "Modelo: Entrop√≠a\n",
      "Accuracy: 0.7837, Precision: 0.8018, Recall: 0.7837, F1-Score: 0.7823\n",
      "\n",
      "Modelo: Entrop√≠a Podado\n",
      "Accuracy: 0.7837, Precision: 0.8018, Recall: 0.7837, F1-Score: 0.7823\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Datos base\n",
    "# ================================================================\n",
    "data_tree_1 = data.copy()\n",
    "X = data_tree_1.drop(\"Workout_Type\", axis=1)\n",
    "y = data_tree_1[\"Workout_Type\"]\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras\n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]  # tres seeds diferentes\n",
    "resultados = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=========== üß† CASO DE PRUEBA {i} (random_state={seed}) ===========\")\n",
    "\n",
    "    # Dividir datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # --- Modelo 1: √Årbol con Gini ---\n",
    "    modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=seed)\n",
    "    modelo_gini.fit(X_train, y_train)\n",
    "    y_pred_gini = modelo_gini.predict(X_test)\n",
    "\n",
    "    # --- Modelo 2: √Årbol con Entrop√≠a ---\n",
    "    modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=seed)\n",
    "    modelo_entropy.fit(X_train, y_train)\n",
    "    y_pred_entropy = modelo_entropy.predict(X_test)\n",
    "\n",
    "    # --- Modelo 3: Entrop√≠a con poda ---\n",
    "    modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=seed)\n",
    "    modelo_entropy_pruned.fit(X_train, y_train)\n",
    "    y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test)\n",
    "\n",
    "    # --- Calcular m√©tricas ---\n",
    "    modelos = {\n",
    "        \"Gini\": y_pred_gini,\n",
    "        \"Entrop√≠a\": y_pred_entropy,\n",
    "        \"Entrop√≠a Podado\": y_pred_entropy_pruned\n",
    "    }\n",
    "\n",
    "    for nombre, y_pred in modelos.items():\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        resultados.append({\n",
    "            \"Caso\": i,\n",
    "            \"Random State\": seed,\n",
    "            \"Modelo\": nombre,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nModelo: {nombre}\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66526514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74fbf08",
   "metadata": {},
   "source": [
    "#### 2. √Årbol de Decisiones - CC:SI - ED:NO - Outliers:NO - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c9b6a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== üß† CASO DE PRUEBA 1 (random_state=111) ===========\n",
      "\n",
      "Modelo: Gini_Balanced\n",
      "Accuracy: 0.7327, Precision: 0.7412, Recall: 0.7327, F1-Score: 0.7350\n",
      "\n",
      "Modelo: Entrop√≠a_Balanced\n",
      "Accuracy: 0.7779, Precision: 0.7923, Recall: 0.7779, F1-Score: 0.7804\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Balanced\n",
      "Accuracy: 0.7779, Precision: 0.7923, Recall: 0.7779, F1-Score: 0.7804\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 2 (random_state=222) ===========\n",
      "\n",
      "Modelo: Gini_Balanced\n",
      "Accuracy: 0.7375, Precision: 0.7338, Recall: 0.7375, F1-Score: 0.7348\n",
      "\n",
      "Modelo: Entrop√≠a_Balanced\n",
      "Accuracy: 0.7740, Precision: 0.7726, Recall: 0.7740, F1-Score: 0.7720\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Balanced\n",
      "Accuracy: 0.7740, Precision: 0.7726, Recall: 0.7740, F1-Score: 0.7720\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 3 (random_state=333) ===========\n",
      "\n",
      "Modelo: Gini_Balanced\n",
      "Accuracy: 0.7365, Precision: 0.7361, Recall: 0.7365, F1-Score: 0.7243\n",
      "\n",
      "Modelo: Entrop√≠a_Balanced\n",
      "Accuracy: 0.7846, Precision: 0.8026, Recall: 0.7846, F1-Score: 0.7833\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Balanced\n",
      "Accuracy: 0.7846, Precision: 0.8026, Recall: 0.7846, F1-Score: 0.7833\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Datos base\n",
    "# ================================================================\n",
    "data_tree_2 = data.copy()\n",
    "X = data_tree_2.drop(\"Workout_Type\", axis=1)\n",
    "y = data_tree_2[\"Workout_Type\"]\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras con class_weight='balanced'\n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]  # Tres seeds diferentes\n",
    "resultados = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=========== üß† CASO DE PRUEBA {i} (random_state={seed}) ===========\")\n",
    "\n",
    "    # Dividir datos (80/20 estratificado)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # --- Modelo 1: √Årbol con Gini (class_weight='balanced') ---\n",
    "    modelo_gini = DecisionTreeClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=5,\n",
    "        random_state=seed,\n",
    "        class_weight='balanced' \n",
    "    )\n",
    "    modelo_gini.fit(X_train, y_train)\n",
    "    y_pred_gini = modelo_gini.predict(X_test)\n",
    "\n",
    "    # --- Modelo 2: √Årbol con Entrop√≠a (class_weight='balanced') ---\n",
    "    modelo_entropy = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=5,\n",
    "        random_state=seed,\n",
    "        class_weight='balanced' \n",
    "    )\n",
    "    modelo_entropy.fit(X_train, y_train)\n",
    "    y_pred_entropy = modelo_entropy.predict(X_test)\n",
    "\n",
    "    # --- Modelo 3: Entrop√≠a con poda (class_weight='balanced') ---\n",
    "    modelo_entropy_pruned = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=5,\n",
    "        min_samples_split=5, # Poda\n",
    "        random_state=seed,\n",
    "        class_weight='balanced' \n",
    "    )\n",
    "    modelo_entropy_pruned.fit(X_train, y_train)\n",
    "    y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test)\n",
    "\n",
    "    # --- Calcular m√©tricas ---\n",
    "    modelos = {\n",
    "        \"Gini_Balanced\": y_pred_gini,\n",
    "        \"Entrop√≠a_Balanced\": y_pred_entropy,\n",
    "        \"Entrop√≠a_Podado_Balanced\": y_pred_entropy_pruned\n",
    "    }\n",
    "\n",
    "    for nombre, y_pred in modelos.items():\n",
    "        # Calcular m√©tricas, usando zero_division=0 para un manejo robusto\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Caso\": i,\n",
    "            \"Random State\": seed,\n",
    "            \"Modelo\": nombre,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "        print(f\"\\nModelo: {nombre}\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1df9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e13b9",
   "metadata": {},
   "source": [
    "#### 3. √Årbol de Decisiones - CC:SI - ED:NO - Outliers:SI - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6028ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o original: (5200, 11)\n",
      "Tama√±o sin outliers: (5055, 11)\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 1 (random_state=111) ===========\n",
      "\n",
      "Modelo: Gini_Clean\n",
      "Accuracy: 0.7399, Precision: 0.7479, Recall: 0.7399, F1-Score: 0.7403\n",
      "\n",
      "Modelo: Entrop√≠a_Clean\n",
      "Accuracy: 0.8042, Precision: 0.8154, Recall: 0.8042, F1-Score: 0.8054\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean\n",
      "Accuracy: 0.8042, Precision: 0.8154, Recall: 0.8042, F1-Score: 0.8054\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 2 (random_state=222) ===========\n",
      "\n",
      "Modelo: Gini_Clean\n",
      "Accuracy: 0.7933, Precision: 0.8097, Recall: 0.7933, F1-Score: 0.7919\n",
      "\n",
      "Modelo: Entrop√≠a_Clean\n",
      "Accuracy: 0.7982, Precision: 0.8104, Recall: 0.7982, F1-Score: 0.8004\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean\n",
      "Accuracy: 0.7982, Precision: 0.8104, Recall: 0.7982, F1-Score: 0.8004\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 3 (random_state=333) ===========\n",
      "\n",
      "Modelo: Gini_Clean\n",
      "Accuracy: 0.7923, Precision: 0.7979, Recall: 0.7923, F1-Score: 0.7928\n",
      "\n",
      "Modelo: Entrop√≠a_Clean\n",
      "Accuracy: 0.8200, Precision: 0.8163, Recall: 0.8200, F1-Score: 0.8177\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean\n",
      "Accuracy: 0.8180, Precision: 0.8149, Recall: 0.8180, F1-Score: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Datos base\n",
    "# ================================================================\n",
    "data_tree_3 = data.copy()\n",
    "\n",
    "# seleccionar solo las columnas num√©ricas\n",
    "num_cols = data_tree_3.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# calcular Q1, Q3 y el rango intercuart√≠lico (IQR)\n",
    "Q1 = data_tree_3[num_cols].quantile(0.25)\n",
    "Q3 = data_tree_3[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# crear una m√°scara booleana que identifique las filas SIN outliers\n",
    "mask = ~((data_tree_3[num_cols] < (Q1 - 1.5 * IQR)) |\n",
    "         (data_tree_3[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# filtrar los datos limpios\n",
    "data_clean = data_tree_3[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o original:\", data_tree_3.shape)\n",
    "print(\"Tama√±o sin outliers:\", data_clean.shape)\n",
    "\n",
    "# Separar X e y con datos limpios\n",
    "X = data_clean.drop(\"Workout_Type\", axis=1)\n",
    "y = data_clean[\"Workout_Type\"]\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras\n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]  \n",
    "resultados = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=========== üß† CASO DE PRUEBA {i} (random_state={seed}) ===========\")\n",
    "\n",
    "    # Dividir datos (80/20 estratificado con el seed actual)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # --- Modelo 1: √Årbol con Gini ---\n",
    "    modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=seed)\n",
    "    modelo_gini.fit(X_train, y_train)\n",
    "    y_pred_gini = modelo_gini.predict(X_test)\n",
    "\n",
    "    # --- Modelo 2: √Årbol con Entrop√≠a ---\n",
    "    modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=seed)\n",
    "    modelo_entropy.fit(X_train, y_train)\n",
    "    y_pred_entropy = modelo_entropy.predict(X_test)\n",
    "\n",
    "    # --- Modelo 3: Entrop√≠a con poda ---\n",
    "    modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=seed)\n",
    "    modelo_entropy_pruned.fit(X_train, y_train)\n",
    "    y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test)\n",
    "\n",
    "    # --- Calcular m√©tricas ---\n",
    "    modelos = {\n",
    "        \"Gini_Clean\": y_pred_gini,\n",
    "        \"Entrop√≠a_Clean\": y_pred_entropy,\n",
    "        \"Entrop√≠a_Podado_Clean\": y_pred_entropy_pruned\n",
    "    }\n",
    "\n",
    "    for nombre, y_pred in modelos.items():\n",
    "        # Calcular m√©tricas (usando zero_division=0 por robustez)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Caso\": i,\n",
    "            \"Random State\": seed,\n",
    "            \"Modelo\": nombre,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "        print(f\"\\nModelo: {nombre}\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "513b99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62fd4e1",
   "metadata": {},
   "source": [
    "#### 4. √Årbol de Decisiones - CC:SI - ED:NO - Outliers:SI - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da4a2c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o original: (5200, 11)\n",
      "Tama√±o sin outliers: (5055, 11)\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 1 (random_state=111) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Balanced\n",
      "Accuracy: 0.7745, Precision: 0.7935, Recall: 0.7745, F1-Score: 0.7710\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Balanced\n",
      "Accuracy: 0.8042, Precision: 0.8154, Recall: 0.8042, F1-Score: 0.8054\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Balanced\n",
      "Accuracy: 0.8042, Precision: 0.8154, Recall: 0.8042, F1-Score: 0.8054\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 2 (random_state=222) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Balanced\n",
      "Accuracy: 0.7933, Precision: 0.8097, Recall: 0.7933, F1-Score: 0.7919\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Balanced\n",
      "Accuracy: 0.7982, Precision: 0.8104, Recall: 0.7982, F1-Score: 0.8004\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Balanced\n",
      "Accuracy: 0.7982, Precision: 0.8104, Recall: 0.7982, F1-Score: 0.8004\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 3 (random_state=333) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Balanced\n",
      "Accuracy: 0.7943, Precision: 0.7994, Recall: 0.7943, F1-Score: 0.7946\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Balanced\n",
      "Accuracy: 0.8190, Precision: 0.8156, Recall: 0.8190, F1-Score: 0.8168\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Balanced\n",
      "Accuracy: 0.8180, Precision: 0.8149, Recall: 0.8180, F1-Score: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Datos base\n",
    "# ================================================================\n",
    "data_tree_4 = data.copy()\n",
    "\n",
    "# seleccionar solo las columnas num√©ricas\n",
    "num_cols = data_tree_4.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# calcular Q1, Q3 y el rango intercuart√≠lico (IQR)\n",
    "Q1 = data_tree_4[num_cols].quantile(0.25)\n",
    "Q3 = data_tree_4[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# crear una m√°scara booleana que identifique las filas SIN outliers\n",
    "mask = ~((data_tree_4[num_cols] < (Q1 - 1.5 * IQR)) |\n",
    "         (data_tree_4[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# filtrar los datos limpios\n",
    "data_clean = data_tree_4[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o original:\", data_tree_4.shape)\n",
    "print(\"Tama√±o sin outliers:\", data_clean.shape)\n",
    "\n",
    "# Separar X e y con datos limpios\n",
    "X = data_clean.drop(\"Workout_Type\", axis=1)\n",
    "y = data_clean[\"Workout_Type\"]\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras \n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]  #\n",
    "resultados = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=========== üß† CASO DE PRUEBA {i} (random_state={seed}) ===========\")\n",
    "\n",
    "    # Dividir datos (80/20 estratificado con el seed actual)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # --- Modelo 1: √Årbol con Gini + Balanced ---\n",
    "    modelo_gini = DecisionTreeClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=5,\n",
    "        random_state=seed,\n",
    "        class_weight='balanced' \n",
    "    )\n",
    "    modelo_gini.fit(X_train, y_train)\n",
    "    y_pred_gini = modelo_gini.predict(X_test)\n",
    "\n",
    "    # --- Modelo 2: √Årbol con Entrop√≠a + Balanced ---\n",
    "    modelo_entropy = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=5,\n",
    "        random_state=seed,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    modelo_entropy.fit(X_train, y_train)\n",
    "    y_pred_entropy = modelo_entropy.predict(X_test)\n",
    "\n",
    "    # --- Modelo 3: Entrop√≠a con poda + Balanced ---\n",
    "    modelo_entropy_pruned = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=5,\n",
    "        min_samples_split=5, # Poda\n",
    "        random_state=seed,\n",
    "        class_weight='balanced' \n",
    "    )\n",
    "    modelo_entropy_pruned.fit(X_train, y_train)\n",
    "    y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test)\n",
    "\n",
    "    # --- Calcular m√©tricas ---\n",
    "    modelos = {\n",
    "        \"Gini_Clean_Balanced\": y_pred_gini,\n",
    "        \"Entrop√≠a_Clean_Balanced\": y_pred_entropy,\n",
    "        \"Entrop√≠a_Podado_Clean_Balanced\": y_pred_entropy_pruned\n",
    "    }\n",
    "\n",
    "    for nombre, y_pred in modelos.items():\n",
    "        # Calcular m√©tricas (usando zero_division=0 por robustez)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Caso\": i,\n",
    "            \"Random State\": seed,\n",
    "            \"Modelo\": nombre,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "        print(f\"\\nModelo: {nombre}\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c517ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb43b18",
   "metadata": {},
   "source": [
    "#### 5. √Årbol de Decisiones - CC:SI - ED:SI - Outliers:NO - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d316eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== üß† CASO DE PRUEBA 1 (random_state=111) ===========\n",
      "\n",
      "Modelo: Gini_Scaled\n",
      "Accuracy: 0.7327, Precision: 0.7412, Recall: 0.7327, F1-Score: 0.7350\n",
      "\n",
      "Modelo: Entrop√≠a_Scaled\n",
      "Accuracy: 0.7779, Precision: 0.7923, Recall: 0.7779, F1-Score: 0.7804\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Scaled\n",
      "Accuracy: 0.7779, Precision: 0.7923, Recall: 0.7779, F1-Score: 0.7804\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 2 (random_state=222) ===========\n",
      "\n",
      "Modelo: Gini_Scaled\n",
      "Accuracy: 0.7452, Precision: 0.7431, Recall: 0.7452, F1-Score: 0.7438\n",
      "\n",
      "Modelo: Entrop√≠a_Scaled\n",
      "Accuracy: 0.7875, Precision: 0.7968, Recall: 0.7875, F1-Score: 0.7892\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Scaled\n",
      "Accuracy: 0.7875, Precision: 0.7968, Recall: 0.7875, F1-Score: 0.7892\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 3 (random_state=333) ===========\n",
      "\n",
      "Modelo: Gini_Scaled\n",
      "Accuracy: 0.7587, Precision: 0.7538, Recall: 0.7587, F1-Score: 0.7559\n",
      "\n",
      "Modelo: Entrop√≠a_Scaled\n",
      "Accuracy: 0.7837, Precision: 0.8018, Recall: 0.7837, F1-Score: 0.7823\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Scaled\n",
      "Accuracy: 0.7837, Precision: 0.8018, Recall: 0.7837, F1-Score: 0.7823\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Datos base\n",
    "# ================================================================\n",
    "data_tree_5 = data.copy()\n",
    "\n",
    "# Se definen las caracter√≠sticas (X) y la variable objetivo (y)\n",
    "X = data_tree_5.drop(\"Workout_Type\", axis=1)\n",
    "y = data_tree_5[\"Workout_Type\"]\n",
    "\n",
    "# Definir el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras \n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]  \n",
    "resultados = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=========== üß† CASO DE PRUEBA {i} (random_state={seed}) ===========\")\n",
    "\n",
    "    # 1. Dividir datos (80/20 estratificado con el seed actual)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # 2. Preprocesamiento: Escalado solo de columnas num√©ricas (excluyendo 'Gender')\n",
    "    numeric_cols = [col for col in X_train.columns if col not in ['Gender']] \n",
    "\n",
    "    # Hacer copias para el escalado\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    # Escalar\n",
    "    X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "    # 3. Entrenamiento y Predicci√≥n de Modelos\n",
    "    \n",
    "    # --- Modelo 1: √Årbol con Gini ---\n",
    "    modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=seed)\n",
    "    modelo_gini.fit(X_train_scaled, y_train)\n",
    "    y_pred_gini = modelo_gini.predict(X_test_scaled)\n",
    "\n",
    "    # --- Modelo 2: √Årbol con Entrop√≠a ---\n",
    "    modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=seed)\n",
    "    modelo_entropy.fit(X_train_scaled, y_train)\n",
    "    y_pred_entropy = modelo_entropy.predict(X_test_scaled)\n",
    "\n",
    "    # --- Modelo 3: Entrop√≠a con poda ---\n",
    "    modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=seed)\n",
    "    modelo_entropy_pruned.fit(X_train_scaled, y_train)\n",
    "    y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test_scaled)\n",
    "\n",
    "    # 4. Calcular m√©tricas\n",
    "    modelos = {\n",
    "        \"Gini_Scaled\": y_pred_gini,\n",
    "        \"Entrop√≠a_Scaled\": y_pred_entropy,\n",
    "        \"Entrop√≠a_Podado_Scaled\": y_pred_entropy_pruned\n",
    "    }\n",
    "\n",
    "    for nombre, y_pred in modelos.items():\n",
    "        # Calcular m√©tricas (usando zero_division=0 por robustez)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Caso\": i,\n",
    "            \"Random State\": seed,\n",
    "            \"Modelo\": nombre,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "        print(f\"\\nModelo: {nombre}\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e3e83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6489eb76",
   "metadata": {},
   "source": [
    "#### 6. √Årbol de Decisiones - CC:SI - ED:SI - Outliers:NO - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86c9b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== üß† CASO DE PRUEBA 1 (random_state=111) ===========\n",
      "\n",
      "Modelo: Gini_Scaled_Balanced\n",
      "Accuracy: 0.7327, Precision: 0.7412, Recall: 0.7327, F1-Score: 0.7350\n",
      "\n",
      "Modelo: Entrop√≠a_Scaled_Balanced\n",
      "Accuracy: 0.7779, Precision: 0.7923, Recall: 0.7779, F1-Score: 0.7804\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Scaled_Balanced\n",
      "Accuracy: 0.7779, Precision: 0.7923, Recall: 0.7779, F1-Score: 0.7804\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 2 (random_state=222) ===========\n",
      "\n",
      "Modelo: Gini_Scaled_Balanced\n",
      "Accuracy: 0.7375, Precision: 0.7338, Recall: 0.7375, F1-Score: 0.7348\n",
      "\n",
      "Modelo: Entrop√≠a_Scaled_Balanced\n",
      "Accuracy: 0.7740, Precision: 0.7726, Recall: 0.7740, F1-Score: 0.7720\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Scaled_Balanced\n",
      "Accuracy: 0.7740, Precision: 0.7726, Recall: 0.7740, F1-Score: 0.7720\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 3 (random_state=333) ===========\n",
      "\n",
      "Modelo: Gini_Scaled_Balanced\n",
      "Accuracy: 0.7365, Precision: 0.7361, Recall: 0.7365, F1-Score: 0.7243\n",
      "\n",
      "Modelo: Entrop√≠a_Scaled_Balanced\n",
      "Accuracy: 0.7846, Precision: 0.8026, Recall: 0.7846, F1-Score: 0.7833\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Scaled_Balanced\n",
      "Accuracy: 0.7846, Precision: 0.8026, Recall: 0.7846, F1-Score: 0.7833\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Datos base\n",
    "# ================================================================\n",
    "data_tree_6 = data.copy()\n",
    "\n",
    "# Se definen las caracter√≠sticas (X) y la variable objetivo (y)\n",
    "X = data_tree_6.drop(\"Workout_Type\", axis=1)\n",
    "y = data_tree_6[\"Workout_Type\"]\n",
    "\n",
    "# Definir el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras \n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]  \n",
    "resultados = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=========== üß† CASO DE PRUEBA {i} (random_state={seed}) ===========\")\n",
    "\n",
    "    # 1. Dividir datos (80/20 estratificado con el seed actual)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # 2. Preprocesamiento: Escalado solo de columnas num√©ricas\n",
    "    numeric_cols = [col for col in X_train.columns if col not in ['Gender']] # Se asume 'Gender' es la √∫nica no num√©rica relevante aqu√≠\n",
    "\n",
    "    # Hacer copias para el escalado\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    # Escalar\n",
    "    X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "    # 3. Entrenamiento y Predicci√≥n de Modelos (con class_weight='balanced')\n",
    "\n",
    "    # --- Modelo 1: √Årbol con Gini + Balanced ---\n",
    "    modelo_gini = DecisionTreeClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=5,\n",
    "        random_state=seed,\n",
    "        class_weight='balanced' \n",
    "    )\n",
    "    modelo_gini.fit(X_train_scaled, y_train)\n",
    "    y_pred_gini = modelo_gini.predict(X_test_scaled)\n",
    "\n",
    "    # --- Modelo 2: √Årbol con Entrop√≠a + Balanced ---\n",
    "    modelo_entropy = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=5,\n",
    "        random_state=seed,\n",
    "        class_weight='balanced' \n",
    "    )\n",
    "    modelo_entropy.fit(X_train_scaled, y_train)\n",
    "    y_pred_entropy = modelo_entropy.predict(X_test_scaled)\n",
    "\n",
    "    # --- Modelo 3: Entrop√≠a con poda + Balanced ---\n",
    "    modelo_entropy_pruned = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=5,\n",
    "        min_samples_split=5, # Poda\n",
    "        random_state=seed,\n",
    "        class_weight='balanced' \n",
    "    )\n",
    "    modelo_entropy_pruned.fit(X_train_scaled, y_train)\n",
    "    y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test_scaled)\n",
    "\n",
    "    # 4. Calcular m√©tricas\n",
    "    modelos = {\n",
    "        \"Gini_Scaled_Balanced\": y_pred_gini,\n",
    "        \"Entrop√≠a_Scaled_Balanced\": y_pred_entropy,\n",
    "        \"Entrop√≠a_Podado_Scaled_Balanced\": y_pred_entropy_pruned\n",
    "    }\n",
    "\n",
    "    for nombre, y_pred in modelos.items():\n",
    "        # Calcular m√©tricas (usando zero_division=0 por robustez)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Caso\": i,\n",
    "            \"Random State\": seed,\n",
    "            \"Modelo\": nombre,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "        print(f\"\\nModelo: {nombre}\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16c0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23e255",
   "metadata": {},
   "source": [
    "#### 7. √Årbol de Decisiones - CC:SI - ED:SI - Outliers:SI - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f175ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o original: (5200, 11)\n",
      "Tama√±o sin outliers: (5055, 11)\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 1 (random_state=111) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Scaled\n",
      "Accuracy: 0.7399, Precision: 0.7479, Recall: 0.7399, F1-Score: 0.7403\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Scaled\n",
      "Accuracy: 0.8042, Precision: 0.8154, Recall: 0.8042, F1-Score: 0.8054\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Scaled\n",
      "Accuracy: 0.8042, Precision: 0.8154, Recall: 0.8042, F1-Score: 0.8054\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 2 (random_state=222) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Scaled\n",
      "Accuracy: 0.7933, Precision: 0.8097, Recall: 0.7933, F1-Score: 0.7919\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Scaled\n",
      "Accuracy: 0.7982, Precision: 0.8104, Recall: 0.7982, F1-Score: 0.8004\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Scaled\n",
      "Accuracy: 0.7982, Precision: 0.8104, Recall: 0.7982, F1-Score: 0.8004\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 3 (random_state=333) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Scaled\n",
      "Accuracy: 0.7923, Precision: 0.7979, Recall: 0.7923, F1-Score: 0.7928\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Scaled\n",
      "Accuracy: 0.8200, Precision: 0.8163, Recall: 0.8200, F1-Score: 0.8177\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Scaled\n",
      "Accuracy: 0.8180, Precision: 0.8149, Recall: 0.8180, F1-Score: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Datos base\n",
    "# ================================================================\n",
    "data_tree_7 = data.copy()\n",
    "\n",
    "# seleccionar solo las columnas num√©ricas\n",
    "num_cols = data_tree_7.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# calcular Q1, Q3 y el rango intercuart√≠lico (IQR)\n",
    "Q1 = data_tree_7[num_cols].quantile(0.25)\n",
    "Q3 = data_tree_7[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# crear una m√°scara booleana que identifique las filas SIN outliers\n",
    "mask = ~((data_tree_7[num_cols] < (Q1 - 1.5 * IQR)) |\n",
    "         (data_tree_7[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# filtrar los datos limpios\n",
    "data_clean = data_tree_7[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o original:\", data_tree_7.shape)\n",
    "print(\"Tama√±o sin outliers:\", data_clean.shape)\n",
    "\n",
    "# Separar X e y con datos limpios\n",
    "X = data_clean.drop(\"Workout_Type\", axis=1)\n",
    "y = data_clean[\"Workout_Type\"]\n",
    "\n",
    "# Definir el escalador\n",
    "scaler = StandardScaler()\n",
    "# Definir columnas a escalar (asumiendo 'Gender' es la √∫nica no num√©rica en X)\n",
    "numeric_cols = [col for col in X.columns if col not in ['Gender']]\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras \n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]  # Tres seeds diferentes\n",
    "resultados = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=========== üß† CASO DE PRUEBA {i} (random_state={seed}) ===========\")\n",
    "\n",
    "    # 1. Dividir datos (80/20 estratificado con el seed actual)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # 2. Preprocesamiento: Escalado\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    # Ajustar y transformar solo las columnas num√©ricas\n",
    "    X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "\n",
    "    # 3. Entrenamiento y Predicci√≥n de Modelos\n",
    "\n",
    "    # --- Modelo 1: √Årbol con Gini ---\n",
    "    modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=seed)\n",
    "    modelo_gini.fit(X_train_scaled, y_train)\n",
    "    y_pred_gini = modelo_gini.predict(X_test_scaled)\n",
    "\n",
    "    # --- Modelo 2: √Årbol con Entrop√≠a ---\n",
    "    modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=seed)\n",
    "    modelo_entropy.fit(X_train_scaled, y_train)\n",
    "    y_pred_entropy = modelo_entropy.predict(X_test_scaled)\n",
    "\n",
    "    # --- Modelo 3: Entrop√≠a con poda ---\n",
    "    modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=seed)\n",
    "    modelo_entropy_pruned.fit(X_train_scaled, y_train)\n",
    "    y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test_scaled)\n",
    "\n",
    "    # 4. Calcular m√©tricas\n",
    "    modelos = {\n",
    "        \"Gini_Clean_Scaled\": y_pred_gini,\n",
    "        \"Entrop√≠a_Clean_Scaled\": y_pred_entropy,\n",
    "        \"Entrop√≠a_Podado_Clean_Scaled\": y_pred_entropy_pruned\n",
    "    }\n",
    "\n",
    "    for nombre, y_pred in modelos.items():\n",
    "        # Calcular m√©tricas (usando zero_division=0 por robustez)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Caso\": i,\n",
    "            \"Random State\": seed,\n",
    "            \"Modelo\": nombre,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "        print(f\"\\nModelo: {nombre}\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52a3bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc5a64",
   "metadata": {},
   "source": [
    "#### 8. √Årbol de Decisiones - CC:SI - ED:SI - Outliers:SI - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e235d4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o original: (5200, 11)\n",
      "Tama√±o sin outliers: (5055, 11)\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 1 (random_state=111) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Scaled_Balanced\n",
      "Accuracy: 0.7745, Precision: 0.7935, Recall: 0.7745, F1-Score: 0.7710\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Scaled_Balanced\n",
      "Accuracy: 0.8042, Precision: 0.8154, Recall: 0.8042, F1-Score: 0.8054\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Scaled_Balanced\n",
      "Accuracy: 0.8042, Precision: 0.8154, Recall: 0.8042, F1-Score: 0.8054\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 2 (random_state=222) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Scaled_Balanced\n",
      "Accuracy: 0.7933, Precision: 0.8097, Recall: 0.7933, F1-Score: 0.7919\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Scaled_Balanced\n",
      "Accuracy: 0.7982, Precision: 0.8104, Recall: 0.7982, F1-Score: 0.8004\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Scaled_Balanced\n",
      "Accuracy: 0.7982, Precision: 0.8104, Recall: 0.7982, F1-Score: 0.8004\n",
      "\n",
      "=========== üß† CASO DE PRUEBA 3 (random_state=333) ===========\n",
      "\n",
      "Modelo: Gini_Clean_Scaled_Balanced\n",
      "Accuracy: 0.7943, Precision: 0.7994, Recall: 0.7943, F1-Score: 0.7946\n",
      "\n",
      "Modelo: Entrop√≠a_Clean_Scaled_Balanced\n",
      "Accuracy: 0.8190, Precision: 0.8156, Recall: 0.8190, F1-Score: 0.8168\n",
      "\n",
      "Modelo: Entrop√≠a_Podado_Clean_Scaled_Balanced\n",
      "Accuracy: 0.8180, Precision: 0.8149, Recall: 0.8180, F1-Score: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Datos base\n",
    "# ================================================================\n",
    "data_tree_8 = data.copy()\n",
    "\n",
    "# 1. Eliminar outliers (IQR)\n",
    "num_cols = data_tree_8.select_dtypes(include=['float64', 'int64']).columns\n",
    "Q1 = data_tree_8[num_cols].quantile(0.25)\n",
    "Q3 = data_tree_8[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "mask = ~((data_tree_8[num_cols] < (Q1 - 1.5 * IQR)) |\n",
    "         (data_tree_8[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "data_clean = data_tree_8[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o original:\", data_tree_8.shape)\n",
    "print(\"Tama√±o sin outliers:\", data_clean.shape)\n",
    "\n",
    "# 2. Separar X e y\n",
    "X = data_clean.drop(\"Workout_Type\", axis=1)\n",
    "y = data_clean[\"Workout_Type\"]\n",
    "\n",
    "# Definir el escalador y las columnas num√©ricas (excluyendo 'Gender')\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [col for col in X.columns if col not in ['Gender']]\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras \n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]  \n",
    "resultados = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=========== üß† CASO DE PRUEBA {i} (random_state={seed}) ===========\")\n",
    "\n",
    "    # 1. Dividir datos (80/20 estratificado con el seed actual)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # 2. Aplicar Escalado (fit/transform en train, transform en test)\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "\n",
    "    # 3. Entrenamiento y Predicci√≥n de Modelos (con class_weight='balanced')\n",
    "\n",
    "    # --- Modelo 1: Gini + Balanced ---\n",
    "    modelo_gini = DecisionTreeClassifier(\n",
    "        criterion='gini',\n",
    "        max_depth=5,\n",
    "        random_state=seed,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    modelo_gini.fit(X_train_scaled, y_train)\n",
    "    y_pred_gini = modelo_gini.predict(X_test_scaled)\n",
    "\n",
    "    # --- Modelo 2: Entrop√≠a + Balanced ---\n",
    "    modelo_entropy = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=5,\n",
    "        random_state=seed,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    modelo_entropy.fit(X_train_scaled, y_train)\n",
    "    y_pred_entropy = modelo_entropy.predict(X_test_scaled)\n",
    "\n",
    "    # --- Modelo 3: Entrop√≠a con poda + Balanced ---\n",
    "    modelo_entropy_pruned = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=5,\n",
    "        min_samples_split=5, # Poda\n",
    "        random_state=seed,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    modelo_entropy_pruned.fit(X_train_scaled, y_train)\n",
    "    y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test_scaled)\n",
    "\n",
    "    # 4. Calcular m√©tricas\n",
    "    modelos = {\n",
    "        \"Gini_Clean_Scaled_Balanced\": y_pred_gini,\n",
    "        \"Entrop√≠a_Clean_Scaled_Balanced\": y_pred_entropy,\n",
    "        \"Entrop√≠a_Podado_Clean_Scaled_Balanced\": y_pred_entropy_pruned\n",
    "    }\n",
    "\n",
    "    for nombre, y_pred in modelos.items():\n",
    "        # Calcular m√©tricas (usando zero_division=0 por robustez)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Caso\": i,\n",
    "            \"Random State\": seed,\n",
    "            \"Modelo\": nombre,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "        print(f\"\\nModelo: {nombre}\")\n",
    "        print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ae62506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a77f2",
   "metadata": {},
   "source": [
    "# K Vecinos M√°s Cercanos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0da7f",
   "metadata": {},
   "source": [
    "#### 1. KNN - CC:SI - ED:NO - Outliers:NO - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8748d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================\n",
      "üß† CASO DE PRUEBA 1 (random_state=111)\n",
      "=================================================\n",
      "üîπ Mejor K (Euclidiana): 1 | Max Acc: 0.7365\n",
      "üîπ Mejor K (Manhattan): 1 | Max Acc: 0.7913\n",
      "\n",
      "--- Evaluaci√≥n Final ---\n",
      "-> Evaluaci√≥n Euclidiana (k=1):\n",
      "Accuracy: 0.7365, Precision: 0.7362, Recall: 0.7365, F1-Score: 0.7362\n",
      "-> Evaluaci√≥n Manhattan (k=1):\n",
      "Accuracy: 0.7913, Precision: 0.7919, Recall: 0.7913, F1-Score: 0.7916\n",
      "\n",
      "=================================================\n",
      "üß† CASO DE PRUEBA 2 (random_state=222)\n",
      "=================================================\n",
      "üîπ Mejor K (Euclidiana): 1 | Max Acc: 0.7423\n",
      "üîπ Mejor K (Manhattan): 1 | Max Acc: 0.7981\n",
      "\n",
      "--- Evaluaci√≥n Final ---\n",
      "-> Evaluaci√≥n Euclidiana (k=1):\n",
      "Accuracy: 0.7423, Precision: 0.7452, Recall: 0.7423, F1-Score: 0.7429\n",
      "-> Evaluaci√≥n Manhattan (k=1):\n",
      "Accuracy: 0.7981, Precision: 0.8021, Recall: 0.7981, F1-Score: 0.7988\n",
      "\n",
      "=================================================\n",
      "üß† CASO DE PRUEBA 3 (random_state=333)\n",
      "=================================================\n",
      "üîπ Mejor K (Euclidiana): 1 | Max Acc: 0.7596\n",
      "üîπ Mejor K (Manhattan): 1 | Max Acc: 0.8115\n",
      "\n",
      "--- Evaluaci√≥n Final ---\n",
      "-> Evaluaci√≥n Euclidiana (k=1):\n",
      "Accuracy: 0.7596, Precision: 0.7602, Recall: 0.7596, F1-Score: 0.7596\n",
      "-> Evaluaci√≥n Manhattan (k=1):\n",
      "Accuracy: 0.8115, Precision: 0.8125, Recall: 0.8115, F1-Score: 0.8116\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Preparaci√≥n de los datos\n",
    "# ================================================================\n",
    "data_knn_1 = data.copy()\n",
    "\n",
    "X = data_knn_1.drop(\"Workout_Type\", axis=1)\n",
    "y = data_knn_1[\"Workout_Type\"]\n",
    "\n",
    "# Definici√≥n de la funci√≥n de evaluaci√≥n\n",
    "def evaluar_modelo(X_train, X_test, y_train, y_test, metric_name, k_value, seed):\n",
    "    \"\"\"Entrena y eval√∫a un modelo KNN.\"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_value, metric=metric_name)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'Random State': seed,\n",
    "        'M√©trica': metric_name,\n",
    "        'k': k_value,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras\n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]\n",
    "k_range = range(1, 100)\n",
    "resultados_finales = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=================================================\")\n",
    "    print(f\"üß† CASO DE PRUEBA {i} (random_state={seed})\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "    # Divisi√≥n de datos (80% entrenamiento, 20% prueba)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # üîé B√∫squeda de mejores K para el split actual\n",
    "    accuracies_euclidean = []\n",
    "    accuracies_manhattan = []\n",
    "\n",
    "    for metric in ['euclidean', 'manhattan']:\n",
    "        for k in k_range:\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_pred = knn.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            if metric == 'euclidean':\n",
    "                accuracies_euclidean.append(acc)\n",
    "            else:\n",
    "                accuracies_manhattan.append(acc)\n",
    "\n",
    "    # Encontrar el mejor K\n",
    "    best_k_euclidean = k_range[accuracies_euclidean.index(max(accuracies_euclidean))]\n",
    "    best_k_manhattan = k_range[accuracies_manhattan.index(max(accuracies_manhattan))]\n",
    "\n",
    "    print(f\"üîπ Mejor K (Euclidiana): {best_k_euclidean} | Max Acc: {max(accuracies_euclidean):.4f}\")\n",
    "    print(f\"üîπ Mejor K (Manhattan): {best_k_manhattan} | Max Acc: {max(accuracies_manhattan):.4f}\")\n",
    "\n",
    "    # üß† Evaluaci√≥n final con los K √≥ptimos\n",
    "    print(\"\\n--- Evaluaci√≥n Final ---\")\n",
    "    \n",
    "    # Evaluar Euclidiana\n",
    "    print(f\"-> Evaluaci√≥n Euclidiana (k={best_k_euclidean}):\")\n",
    "    resultados_finales.append(\n",
    "        evaluar_modelo(X_train, X_test, y_train, y_test, 'euclidean', best_k_euclidean, seed)\n",
    "    )\n",
    "    \n",
    "    # Evaluar Manhattan\n",
    "    print(f\"-> Evaluaci√≥n Manhattan (k={best_k_manhattan}):\")\n",
    "    resultados_finales.append(\n",
    "        evaluar_modelo(X_train, X_test, y_train, y_test, 'manhattan', best_k_manhattan, seed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la gr√°fica de knn con el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d2508",
   "metadata": {},
   "source": [
    "#### 2. KNN - CC:SI - ED:NO - Outliers:NO - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "007f3c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================\n",
      "üß† CASO DE PRUEBA 1 (random_state=111) - Balanced\n",
      "=================================================\n",
      "üîπ Mejor K (Euclidiana): 1 | Max Acc: 0.7365\n",
      "üîπ Mejor K (Manhattan): 1 | Max Acc: 0.7913\n",
      "\n",
      "--- Evaluaci√≥n Final ---\n",
      "-> Evaluaci√≥n Euclidiana (k=1, weights='distance'):\n",
      "Accuracy: 0.7365, Precision: 0.7362, Recall: 0.7365, F1-Score: 0.7362\n",
      "-> Evaluaci√≥n Manhattan (k=1, weights='distance'):\n",
      "Accuracy: 0.7913, Precision: 0.7919, Recall: 0.7913, F1-Score: 0.7916\n",
      "\n",
      "=================================================\n",
      "üß† CASO DE PRUEBA 2 (random_state=222) - Balanced\n",
      "=================================================\n",
      "üîπ Mejor K (Euclidiana): 1 | Max Acc: 0.7423\n",
      "üîπ Mejor K (Manhattan): 1 | Max Acc: 0.7981\n",
      "\n",
      "--- Evaluaci√≥n Final ---\n",
      "-> Evaluaci√≥n Euclidiana (k=1, weights='distance'):\n",
      "Accuracy: 0.7423, Precision: 0.7452, Recall: 0.7423, F1-Score: 0.7429\n",
      "-> Evaluaci√≥n Manhattan (k=1, weights='distance'):\n",
      "Accuracy: 0.7981, Precision: 0.8021, Recall: 0.7981, F1-Score: 0.7988\n",
      "\n",
      "=================================================\n",
      "üß† CASO DE PRUEBA 3 (random_state=333) - Balanced\n",
      "=================================================\n",
      "üîπ Mejor K (Euclidiana): 1 | Max Acc: 0.7596\n",
      "üîπ Mejor K (Manhattan): 1 | Max Acc: 0.8115\n",
      "\n",
      "--- Evaluaci√≥n Final ---\n",
      "-> Evaluaci√≥n Euclidiana (k=1, weights='distance'):\n",
      "Accuracy: 0.7596, Precision: 0.7602, Recall: 0.7596, F1-Score: 0.7596\n",
      "-> Evaluaci√≥n Manhattan (k=1, weights='distance'):\n",
      "Accuracy: 0.8115, Precision: 0.8125, Recall: 0.8115, F1-Score: 0.8116\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üìÇ Preparaci√≥n de los datos\n",
    "# ================================================================\n",
    "data_knn_2 = data.copy()\n",
    "\n",
    "X = data_knn_2.drop(\"Workout_Type\", axis=1)\n",
    "y = data_knn_2[\"Workout_Type\"]\n",
    "\n",
    "# Definici√≥n de la funci√≥n de evaluaci√≥n\n",
    "def evaluar_modelo(X_train, X_test, y_train, y_test, metric_name, k_value, seed):\n",
    "    \"\"\"Entrena y eval√∫a un modelo KNN usando weights='distance'.\"\"\"\n",
    "    # weights='distance' prioriza los vecinos m√°s cercanos, actuando como un balanceo ponderado.\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_value, metric=metric_name, weights='distance')\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'Random State': seed,\n",
    "        'M√©trica': metric_name,\n",
    "        'k': k_value,\n",
    "        'Weights': 'distance',\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "\n",
    "# ================================================================\n",
    "# üîÅ Tres muestras \n",
    "# ================================================================\n",
    "random_states = [111, 222, 333]\n",
    "k_range = range(1, 100)\n",
    "resultados_finales = []\n",
    "\n",
    "for i, seed in enumerate(random_states, start=1):\n",
    "    print(f\"\\n=================================================\")\n",
    "    print(f\"üß† CASO DE PRUEBA {i} (random_state={seed}) - Balanced\")\n",
    "    print(f\"=================================================\")\n",
    "\n",
    "    # Divisi√≥n de datos (80% entrenamiento, 20% prueba)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # üîé B√∫squeda de mejores K para el split actual (usando weights='distance')\n",
    "    accuracies_euclidean = []\n",
    "    accuracies_manhattan = []\n",
    "\n",
    "    for metric in ['euclidean', 'manhattan']:\n",
    "        for k in k_range:\n",
    "            # Importante: Usamos weights='distance' en la b√∫squeda de K\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_pred = knn.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            if metric == 'euclidean':\n",
    "                accuracies_euclidean.append(acc)\n",
    "            else:\n",
    "                accuracies_manhattan.append(acc)\n",
    "\n",
    "    # Encontrar el mejor K\n",
    "    best_k_euclidean = k_range[accuracies_euclidean.index(max(accuracies_euclidean))]\n",
    "    best_k_manhattan = k_range[accuracies_manhattan.index(max(accuracies_manhattan))]\n",
    "\n",
    "    print(f\"üîπ Mejor K (Euclidiana): {best_k_euclidean} | Max Acc: {max(accuracies_euclidean):.4f}\")\n",
    "    print(f\"üîπ Mejor K (Manhattan): {best_k_manhattan} | Max Acc: {max(accuracies_manhattan):.4f}\")\n",
    "\n",
    "    # üß† Evaluaci√≥n final con los K √≥ptimos\n",
    "    print(\"\\n--- Evaluaci√≥n Final ---\")\n",
    "    \n",
    "    # Evaluar Euclidiana\n",
    "    print(f\"-> Evaluaci√≥n Euclidiana (k={best_k_euclidean}, weights='distance'):\")\n",
    "    resultados_finales.append(\n",
    "        evaluar_modelo(X_train, X_test, y_train, y_test, 'euclidean', best_k_euclidean, seed)\n",
    "    )\n",
    "    \n",
    "    # Evaluar Manhattan\n",
    "    print(f\"-> Evaluaci√≥n Manhattan (k={best_k_manhattan}, weights='distance'):\")\n",
    "    resultados_finales.append(\n",
    "        evaluar_modelo(X_train, X_test, y_train, y_test, 'manhattan', best_k_manhattan, seed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la gr√°fica de knn con el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37249453",
   "metadata": {},
   "source": [
    "#### 3. KNN - CC:SI - ED:NO - Outliers:SI - Balanceo: NO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
