{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4340e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librer√≠as necesarias\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# diccionario para guardar todas las m√©tricas\n",
    "metricas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86804070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight_kg</th>\n",
       "      <th>Height_m</th>\n",
       "      <th>Max_BPM</th>\n",
       "      <th>Avg_BPM</th>\n",
       "      <th>Resting_BPM</th>\n",
       "      <th>Session_Duration_hours</th>\n",
       "      <th>Calories_Burned</th>\n",
       "      <th>Workout_Type</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.14</td>\n",
       "      <td>101.05</td>\n",
       "      <td>1.95</td>\n",
       "      <td>171.17</td>\n",
       "      <td>130.81</td>\n",
       "      <td>68.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>959.43</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.17</td>\n",
       "      <td>41.63</td>\n",
       "      <td>1.78</td>\n",
       "      <td>167.33</td>\n",
       "      <td>158.46</td>\n",
       "      <td>63.95</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1424.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.07</td>\n",
       "      <td>63.81</td>\n",
       "      <td>1.78</td>\n",
       "      <td>187.86</td>\n",
       "      <td>137.11</td>\n",
       "      <td>60.93</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1766.64</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.30</td>\n",
       "      <td>59.77</td>\n",
       "      <td>1.78</td>\n",
       "      <td>183.83</td>\n",
       "      <td>120.32</td>\n",
       "      <td>60.01</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1028.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.99</td>\n",
       "      <td>57.60</td>\n",
       "      <td>1.56</td>\n",
       "      <td>166.25</td>\n",
       "      <td>151.82</td>\n",
       "      <td>67.97</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1295.80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Weight_kg  Height_m  Max_BPM  Avg_BPM  Resting_BPM  \\\n",
       "0  21.14     101.05      1.95   171.17   130.81        68.96   \n",
       "1  44.17      41.63      1.78   167.33   158.46        63.95   \n",
       "2  20.07      63.81      1.78   187.86   137.11        60.93   \n",
       "3  36.30      59.77      1.78   183.83   120.32        60.01   \n",
       "4  51.99      57.60      1.56   166.25   151.82        67.97   \n",
       "\n",
       "   Session_Duration_hours  Calories_Burned  Workout_Type  Gender_Female  \\\n",
       "0                    0.97           959.43             2            0.0   \n",
       "1                    1.48          1424.35             0            0.0   \n",
       "2                    1.70          1766.64             0            1.0   \n",
       "3                    0.85          1028.50             1            1.0   \n",
       "4                    1.66          1295.80             3            0.0   \n",
       "\n",
       "   Gender_Male  \n",
       "0          1.0  \n",
       "1          1.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          1.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Carga del Conjunto de Datos ---\n",
    "data = pd.read_csv(\"../data/subset/clean_subset_lifestyledata_rows5200_seed5200.csv\")\n",
    "\n",
    "# --- 2. Codificaci√≥n de Etiquetas (Label Encoding) ---\n",
    "label_encoder = LabelEncoder()\n",
    "# Se transforma la variable objetivo 'Workout_Type' a valores num√©ricos.\n",
    "data['Workout_Type'] = label_encoder.fit_transform(data['Workout_Type'])\n",
    "\n",
    "# --- 3. Codificaci√≥n One-Hot (One-Hot Encoding) ---\n",
    "# Se define la lista de columnas categ√≥ricas nominales a transformar.\n",
    "nominal_cols = ['Gender']\n",
    "# sparse_output=False: Devuelve una matriz densa (array de NumPy) en lugar de una dispersa.\n",
    "# handle_unknown='ignore': Si aparece una categor√≠a no vista durante la transformaci√≥n, la ignora.\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "# Esto crea nuevas columnas binarias para cada categor√≠a.\n",
    "encoded = ohe.fit_transform(data[nominal_cols])\n",
    "# Se convierte la matriz resultante en un DataFrame con nombres de columna apropiados.\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(nominal_cols))\n",
    "\n",
    "# --- 4. Combinaci√≥n de los Datos Procesados ---\n",
    "# Se elimina la columna original 'Gender' del DataFrame principal.\n",
    "# reset_index(drop=True) asegura que los √≠ndices se alineen correctamente para la concatenaci√≥n.\n",
    "data = data.drop(columns=nominal_cols).reset_index(drop=True)\n",
    "encoded_df = encoded_df.reset_index(drop=True)\n",
    "\n",
    "# Se concatenan el DataFrame original y el nuevo DataFrame con las columnas codificadas.\n",
    "# axis=1 indica que la uni√≥n se realiza por columnas.\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "# --- 5. Visualizaci√≥n ---\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d8ea8",
   "metadata": {},
   "source": [
    "# √Årbol de Decisiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928020a",
   "metadata": {},
   "source": [
    "#### 1. √Årbol de Decisiones - CC:SI - ED:NO - Outliers:NO - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5b46c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©tricas del Modelo Gini:\n",
      "Accuracy: 0.7356, Precision: 0.7395, Recall: 0.7356, F1-Score: 0.7361\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a:\n",
      "Accuracy: 0.7750, Precision: 0.7953, Recall: 0.7750, F1-Score: 0.7820\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a Podado:\n",
      "Accuracy: 0.7750, Precision: 0.7953, Recall: 0.7750, F1-Score: 0.7820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datos a utilizar\n",
    "data_model_1 = data.copy()\n",
    "\n",
    "# Se definen las caracter√≠sticas (X) y la variable objetivo (y)\n",
    "# X contiene todas las columnas EXCEPTO 'Workout_Type'.\n",
    "X = data_model_1.drop(\"Workout_Type\", axis=1)\n",
    "\n",
    "# y contiene √öNICAMENTE la columna 'Workout_Type'.\n",
    "y = data_model_1[\"Workout_Type\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5200, stratify=y)\n",
    "\n",
    "# Se crea un √°rbol con el criterio de impureza de Gini y una profundidad m√°xima de 5\n",
    "modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=5200)\n",
    "# Se entrena el modelo con los datos de entrenamiento\n",
    "modelo_gini.fit(X_train, y_train)\n",
    "\n",
    "# Se crea un √°rbol con el criterio de Entrop√≠a\n",
    "modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=5200)\n",
    "# Se entrena el modelo\n",
    "modelo_entropy.fit(X_train, y_train)\n",
    "\n",
    "# Se crea un √°rbol con Entrop√≠a y una condici√≥n de poda adicional\n",
    "modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=5200)\n",
    "# Se entrena el modelo\n",
    "modelo_entropy_pruned.fit(X_train, y_train)\n",
    "\n",
    "# Predecir con el modelo Gini\n",
    "y_pred_gini = modelo_gini.predict(X_test)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a\n",
    "y_pred_entropy = modelo_entropy.predict(X_test)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a podado\n",
    "y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test)\n",
    "\n",
    "# M√©tricas para el modelo Gini (accuracy, precision, recall, f1-score)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a podado (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy_pruned = accuracy_score(y_test, y_pred_entropy_pruned)\n",
    "precision_entropy_pruned = precision_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "recall_entropy_pruned = recall_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "f1_entropy_pruned = f1_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "\n",
    "# Mostrar las m√©tricas\n",
    "print(\"M√©tricas del Modelo Gini:\")\n",
    "print(f\"Accuracy: {accuracy_gini:.4f}, Precision: {precision_gini:.4f}, Recall: {recall_gini:.4f}, F1-Score: {f1_gini:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a:\")\n",
    "print(f\"Accuracy: {accuracy_entropy:.4f}, Precision: {precision_entropy:.4f}, Recall: {recall_entropy:.4f}, F1-Score: {f1_entropy:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a Podado:\")\n",
    "print(f\"Accuracy: {accuracy_entropy_pruned:.4f}, Precision: {precision_entropy_pruned:.4f}, Recall: {recall_entropy_pruned:.4f}, F1-Score: {f1_entropy_pruned:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66526514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508aa515",
   "metadata": {},
   "source": [
    "## Conclusi√≥n:\n",
    "\n",
    "Se evaluaron tres modelos de √°rbol de decisi√≥n: uno con Gini, otro con entrop√≠a y un tercero con entrop√≠a podado. Los dos modelos con entrop√≠a obtuvieron mejores resultados, con una precisi√≥n cercana al 78% y un F1-score de 0.78. Se seleccion√≥ el modelo con entrop√≠a podado por ofrecer el mismo nivel de exactitud que el no podado, pero con menor complejidad y mejor capacidad de generalizaci√≥n, logrando un equilibrio √≥ptimo entre rendimiento y simplicidad para predecir el tipo de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74fbf08",
   "metadata": {},
   "source": [
    "#### 2. √Årbol de Decisiones - CC:SI - ED:NO - Outliers:NO - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c9b6a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©tricas del Modelo Gini:\n",
      "Accuracy: 0.7337, Precision: 0.7392, Recall: 0.7337, F1-Score: 0.7348\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a:\n",
      "Accuracy: 0.7750, Precision: 0.7953, Recall: 0.7750, F1-Score: 0.7820\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a Podado:\n",
      "Accuracy: 0.7750, Precision: 0.7953, Recall: 0.7750, F1-Score: 0.7820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datos a utilizar\n",
    "data_model_2 = data.copy()\n",
    "\n",
    "# Se definen las caracter√≠sticas (X) y la variable objetivo (y)\n",
    "# X contiene todas las columnas EXCEPTO 'Workout_Type'.\n",
    "X = data_model_2.drop(\"Workout_Type\", axis=1)\n",
    "\n",
    "# y contiene √öNICAMENTE la columna 'Workout_Type'.\n",
    "y = data_model_2[\"Workout_Type\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5200, stratify=y)\n",
    "\n",
    "# Se crea un √°rbol con el criterio de impureza de Gini y una profundidad m√°xima de 5\n",
    "modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo con los datos de entrenamiento\n",
    "modelo_gini.fit(X_train, y_train)\n",
    "\n",
    "# Se crea un √°rbol con el criterio de Entrop√≠a\n",
    "modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo\n",
    "modelo_entropy.fit(X_train, y_train)\n",
    "\n",
    "# Se crea un √°rbol con Entrop√≠a y una condici√≥n de poda adicional\n",
    "modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo\n",
    "modelo_entropy_pruned.fit(X_train, y_train)\n",
    "    \n",
    "# Predecir con el modelo Gini\n",
    "y_pred_gini = modelo_gini.predict(X_test)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a\n",
    "y_pred_entropy = modelo_entropy.predict(X_test)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a podado\n",
    "y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test)\n",
    "\n",
    "# M√©tricas para el modelo Gini (accuracy, precision, recall, f1-score)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a podado (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy_pruned = accuracy_score(y_test, y_pred_entropy_pruned)\n",
    "precision_entropy_pruned = precision_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "recall_entropy_pruned = recall_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "f1_entropy_pruned = f1_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "\n",
    "# Mostrar las m√©tricas\n",
    "print(\"M√©tricas del Modelo Gini:\")\n",
    "print(f\"Accuracy: {accuracy_gini:.4f}, Precision: {precision_gini:.4f}, Recall: {recall_gini:.4f}, F1-Score: {f1_gini:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a:\")\n",
    "print(f\"Accuracy: {accuracy_entropy:.4f}, Precision: {precision_entropy:.4f}, Recall: {recall_entropy:.4f}, F1-Score: {f1_entropy:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a Podado:\")\n",
    "print(f\"Accuracy: {accuracy_entropy_pruned:.4f}, Precision: {precision_entropy_pruned:.4f}, Recall: {recall_entropy_pruned:.4f}, F1-Score: {f1_entropy_pruned:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b1df9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6cc00",
   "metadata": {},
   "source": [
    "## Conclusi√≥n:\n",
    "\n",
    "A pesar de aplicar el balanceo de clases mediante el par√°metro class_weight='balanced', los resultados obtenidos fueron muy similares a los del modelo sin balancear. Esto indica que el conjunto de datos ya presentaba una distribuci√≥n de clases relativamente equilibrada, por lo que el ajuste autom√°tico de pesos no gener√≥ cambios significativos en las m√©tricas de desempe√±o. En consecuencia, el modelo mantiene su capacidad predictiva sin necesidad de un rebalanceo adicional, conservando una precisi√≥n y un F1-score estables alrededor del 0.78 en el modelo de entrop√≠a con podado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e13b9",
   "metadata": {},
   "source": [
    "#### 3. √Årbol de Decisiones - CC:SI - ED:NO - Outliers:SI - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6028ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o original: (5200, 11)\n",
      "Tama√±o sin outliers: (5055, 11)\n",
      "\n",
      "M√©tricas del Modelo Gini:\n",
      "Accuracy: 0.7883, Precision: 0.8018, Recall: 0.7883, F1-Score: 0.7908\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a:\n",
      "Accuracy: 0.7943, Precision: 0.8060, Recall: 0.7943, F1-Score: 0.7945\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a Podado:\n",
      "Accuracy: 0.7943, Precision: 0.8060, Recall: 0.7943, F1-Score: 0.7945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datos a utilizar\n",
    "data_model_3 = data.copy()\n",
    "\n",
    "# ==========================\n",
    "# Eliminar outliers (IQR)\n",
    "# ==========================\n",
    "\n",
    "# seleccionar solo las columnas num√©ricas\n",
    "num_cols = data_model_3.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# calcular Q1, Q3 y el rango intercuart√≠lico (IQR)\n",
    "Q1 = data_model_3[num_cols].quantile(0.25)\n",
    "Q3 = data_model_3[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# crear una m√°scara booleana que identifique las filas SIN outliers\n",
    "mask = ~((data_model_3[num_cols] < (Q1 - 1.5 * IQR)) |\n",
    "         (data_model_3[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# filtrar los datos limpios\n",
    "data_clean = data_model_3[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o original:\", data_model_3.shape)\n",
    "print(\"Tama√±o sin outliers:\", data_clean.shape)\n",
    "\n",
    "# ==========================\n",
    "# Separar X e y\n",
    "# ==========================\n",
    "X = data_clean.drop(\"Workout_Type\", axis=1)\n",
    "y = data_clean[\"Workout_Type\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=5200, stratify=y\n",
    ")\n",
    "\n",
    "# √Årbol con criterio Gini\n",
    "modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=5200)\n",
    "modelo_gini.fit(X_train, y_train)\n",
    "\n",
    "# √Årbol con criterio Entrop√≠a\n",
    "modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=5200)\n",
    "modelo_entropy.fit(X_train, y_train)\n",
    "\n",
    "# √Årbol con Entrop√≠a y poda\n",
    "modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=5200)\n",
    "modelo_entropy_pruned.fit(X_train, y_train)\n",
    "\n",
    "# predicciones\n",
    "y_pred_gini = modelo_gini.predict(X_test)\n",
    "y_pred_entropy = modelo_entropy.predict(X_test)\n",
    "y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test)\n",
    "\n",
    "# m√©tricas\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "accuracy_entropy_pruned = accuracy_score(y_test, y_pred_entropy_pruned)\n",
    "precision_entropy_pruned = precision_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "recall_entropy_pruned = recall_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "f1_entropy_pruned = f1_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "\n",
    "# ==========================\n",
    "# Mostrar resultados\n",
    "# ==========================\n",
    "print(\"\\nM√©tricas del Modelo Gini:\")\n",
    "print(f\"Accuracy: {accuracy_gini:.4f}, Precision: {precision_gini:.4f}, Recall: {recall_gini:.4f}, F1-Score: {f1_gini:.4f}\\n\")\n",
    "\n",
    "print(\"M√©tricas del Modelo Entrop√≠a:\")\n",
    "print(f\"Accuracy: {accuracy_entropy:.4f}, Precision: {precision_entropy:.4f}, Recall: {recall_entropy:.4f}, F1-Score: {f1_entropy:.4f}\\n\")\n",
    "\n",
    "print(\"M√©tricas del Modelo Entrop√≠a Podado:\")\n",
    "print(f\"Accuracy: {accuracy_entropy_pruned:.4f}, Precision: {precision_entropy_pruned:.4f}, Recall: {recall_entropy_pruned:.4f}, F1-Score: {f1_entropy_pruned:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "513b99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf2e0a",
   "metadata": {},
   "source": [
    "## Conclusi√≥n:\n",
    "\n",
    "La eliminaci√≥n de outliers mejor√≥ las m√©tricas del modelo, indicando que los datos at√≠picos afectaban su desempe√±o. Al limpiar el conjunto de datos, los √°rboles de decisi√≥n lograron una clasificaci√≥n m√°s precisa y estable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62fd4e1",
   "metadata": {},
   "source": [
    "#### 4. √Årbol de Decisiones - CC:SI - ED:NO - Outliers:SI - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da4a2c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o original: (5200, 11)\n",
      "Tama√±o sin outliers: (5055, 11)\n",
      "\n",
      "M√©tricas del Modelo Gini:\n",
      "Accuracy: 0.7883, Precision: 0.8018, Recall: 0.7883, F1-Score: 0.7908\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a:\n",
      "Accuracy: 0.8002, Precision: 0.8114, Recall: 0.8002, F1-Score: 0.8018\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a Podado:\n",
      "Accuracy: 0.8002, Precision: 0.8114, Recall: 0.8002, F1-Score: 0.8018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datos a utilizar\n",
    "data_model_4 = data.copy()\n",
    "\n",
    "# ==========================\n",
    "# Eliminar outliers (IQR)\n",
    "# ==========================\n",
    "\n",
    "# seleccionar solo las columnas num√©ricas\n",
    "num_cols = data_model_4.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# calcular Q1, Q3 y el rango intercuart√≠lico (IQR)\n",
    "Q1 = data_model_4[num_cols].quantile(0.25)\n",
    "Q3 = data_model_4[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# crear una m√°scara booleana que identifique las filas SIN outliers\n",
    "mask = ~((data_model_4[num_cols] < (Q1 - 1.5 * IQR)) |\n",
    "         (data_model_4[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# filtrar los datos limpios\n",
    "data_clean = data_model_4[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o original:\", data_model_4.shape)\n",
    "print(\"Tama√±o sin outliers:\", data_clean.shape)\n",
    "\n",
    "# ==========================\n",
    "# Separar X e y\n",
    "# ==========================\n",
    "X = data_clean.drop(\"Workout_Type\", axis=1)\n",
    "y = data_clean[\"Workout_Type\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=5200, stratify=y\n",
    ")\n",
    "\n",
    "# √Årbol con criterio Gini\n",
    "modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=5200, class_weight='balanced')\n",
    "modelo_gini.fit(X_train, y_train)\n",
    "\n",
    "# √Årbol con criterio Entrop√≠a\n",
    "modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=5200, class_weight='balanced')\n",
    "modelo_entropy.fit(X_train, y_train)\n",
    "\n",
    "# √Årbol con Entrop√≠a y poda\n",
    "modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=5200, class_weight='balanced')\n",
    "modelo_entropy_pruned.fit(X_train, y_train)\n",
    "\n",
    "# predicciones\n",
    "y_pred_gini = modelo_gini.predict(X_test)\n",
    "y_pred_entropy = modelo_entropy.predict(X_test)\n",
    "y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test)\n",
    "\n",
    "# m√©tricas\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "accuracy_entropy_pruned = accuracy_score(y_test, y_pred_entropy_pruned)\n",
    "precision_entropy_pruned = precision_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "recall_entropy_pruned = recall_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "f1_entropy_pruned = f1_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "\n",
    "# ==========================\n",
    "# Mostrar resultados\n",
    "# ==========================\n",
    "print(\"\\nM√©tricas del Modelo Gini:\")\n",
    "print(f\"Accuracy: {accuracy_gini:.4f}, Precision: {precision_gini:.4f}, Recall: {recall_gini:.4f}, F1-Score: {f1_gini:.4f}\\n\")\n",
    "\n",
    "print(\"M√©tricas del Modelo Entrop√≠a:\")\n",
    "print(f\"Accuracy: {accuracy_entropy:.4f}, Precision: {precision_entropy:.4f}, Recall: {recall_entropy:.4f}, F1-Score: {f1_entropy:.4f}\\n\")\n",
    "\n",
    "print(\"M√©tricas del Modelo Entrop√≠a Podado:\")\n",
    "print(f\"Accuracy: {accuracy_entropy_pruned:.4f}, Precision: {precision_entropy_pruned:.4f}, Recall: {recall_entropy_pruned:.4f}, F1-Score: {f1_entropy_pruned:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c517ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1ff89",
   "metadata": {},
   "source": [
    "## Conclusi√≥n:\n",
    "\n",
    "Ahora, con el balanceo de clases y la eliminaci√≥n de outliers, el modelo mostr√≥ una mejora notable en sus m√©tricas.\n",
    "Por ejemplo, el modelo con entrop√≠a alcanz√≥ un accuracy de 0.8002, precisi√≥n de 0.8114, recall de 0.8002 y un F1-Score de 0.8018, superando los valores anteriores.\n",
    "Esto demuestra que al equilibrar las clases y eliminar datos at√≠picos, el √°rbol de decisi√≥n logra una clasificaci√≥n m√°s precisa y estable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb43b18",
   "metadata": {},
   "source": [
    "#### 5. √Årbol de Decisiones - CC:SI - ED:SI - Outliers:NO - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d316eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©tricas del Modelo Gini:\n",
      "Accuracy: 0.7356, Precision: 0.7395, Recall: 0.7356, F1-Score: 0.7361\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a:\n",
      "Accuracy: 0.7750, Precision: 0.7953, Recall: 0.7750, F1-Score: 0.7820\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a Podado:\n",
      "Accuracy: 0.7750, Precision: 0.7953, Recall: 0.7750, F1-Score: 0.7820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datos a utilizar\n",
    "data_model_5 = data.copy()\n",
    "\n",
    "# Se definen las caracter√≠sticas (X) y la variable objetivo (y)\n",
    "# X contiene todas las columnas EXCEPTO 'Workout_Type'.\n",
    "X = data_model_5.drop(\"Workout_Type\", axis=1)\n",
    "\n",
    "# y contiene √öNICAMENTE la columna 'Workout_Type'.\n",
    "y = data_model_5[\"Workout_Type\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5200, stratify=y)\n",
    "\n",
    "# üîπ Escalado de las caracter√≠sticas (excepto 'Gender' y 'Workout_Type')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Seleccionar columnas num√©ricas que no sean categ√≥ricas\n",
    "numeric_cols = [col for col in X_train.columns if col not in ['Gender', 'Workout_Type']]\n",
    "\n",
    "# Hacer copias para no perder las columnas categ√≥ricas\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Escalar solo las columnas num√©ricas\n",
    "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Se crea un √°rbol con el criterio de impureza de Gini y una profundidad m√°xima de 5\n",
    "modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=5200)\n",
    "# Se entrena el modelo con los datos de entrenamiento\n",
    "modelo_gini.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Se crea un √°rbol con el criterio de Entrop√≠a\n",
    "modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=5200)\n",
    "# Se entrena el modelo\n",
    "modelo_entropy.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Se crea un √°rbol con Entrop√≠a y una condici√≥n de poda adicional\n",
    "modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=5200)\n",
    "# Se entrena el modelo\n",
    "modelo_entropy_pruned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predecir con el modelo Gini\n",
    "y_pred_gini = modelo_gini.predict(X_test_scaled)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a\n",
    "y_pred_entropy = modelo_entropy.predict(X_test_scaled)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a podado\n",
    "y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas para el modelo Gini (accuracy, precision, recall, f1-score)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a podado (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy_pruned = accuracy_score(y_test, y_pred_entropy_pruned)\n",
    "precision_entropy_pruned = precision_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "recall_entropy_pruned = recall_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "f1_entropy_pruned = f1_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "\n",
    "# Mostrar las m√©tricas\n",
    "print(\"M√©tricas del Modelo Gini:\")\n",
    "print(f\"Accuracy: {accuracy_gini:.4f}, Precision: {precision_gini:.4f}, Recall: {recall_gini:.4f}, F1-Score: {f1_gini:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a:\")\n",
    "print(f\"Accuracy: {accuracy_entropy:.4f}, Precision: {precision_entropy:.4f}, Recall: {recall_entropy:.4f}, F1-Score: {f1_entropy:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a Podado:\")\n",
    "print(f\"Accuracy: {accuracy_entropy_pruned:.4f}, Precision: {precision_entropy_pruned:.4f}, Recall: {recall_entropy_pruned:.4f}, F1-Score: {f1_entropy_pruned:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b819eb1f",
   "metadata": {},
   "source": [
    "## Conclusi√≥n:\n",
    "\n",
    "En este caso, la normalizaci√≥n de los datos no afect√≥ el rendimiento del modelo, ya que los √°rboles de decisi√≥n no dependen de las escalas de las variables.\n",
    "Esto se debe a que los √°rboles no calculan distancias ni magnitudes, sino que realizan divisiones basadas en umbrales de los valores de cada atributo para separar las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6489eb76",
   "metadata": {},
   "source": [
    "#### 6. √Årbol de Decisiones - CC:SI - ED:SI - Outliers:NO - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86c9b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©tricas del Modelo Gini:\n",
      "Accuracy: 0.7337, Precision: 0.7392, Recall: 0.7337, F1-Score: 0.7348\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a:\n",
      "Accuracy: 0.7750, Precision: 0.7953, Recall: 0.7750, F1-Score: 0.7820\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a Podado:\n",
      "Accuracy: 0.7750, Precision: 0.7953, Recall: 0.7750, F1-Score: 0.7820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datos a utilizar\n",
    "data_model_6 = data.copy()\n",
    "\n",
    "# Se definen las caracter√≠sticas (X) y la variable objetivo (y)\n",
    "# X contiene todas las columnas EXCEPTO 'Workout_Type'.\n",
    "X = data_model_6.drop(\"Workout_Type\", axis=1)\n",
    "\n",
    "# y contiene √öNICAMENTE la columna 'Workout_Type'.\n",
    "y = data_model_6[\"Workout_Type\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5200, stratify=y)\n",
    "\n",
    "# üîπ Escalado de las caracter√≠sticas (excepto 'Gender' y 'Workout_Type')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Seleccionar columnas num√©ricas que no sean categ√≥ricas\n",
    "numeric_cols = [col for col in X_train.columns if col not in ['Gender', 'Workout_Type']]\n",
    "\n",
    "# Hacer copias para no perder las columnas categ√≥ricas\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Escalar solo las columnas num√©ricas\n",
    "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Se crea un √°rbol con el criterio de impureza de Gini y una profundidad m√°xima de 5\n",
    "modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo con los datos de entrenamiento\n",
    "modelo_gini.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Se crea un √°rbol con el criterio de Entrop√≠a\n",
    "modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo\n",
    "modelo_entropy.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Se crea un √°rbol con Entrop√≠a y una condici√≥n de poda adicional\n",
    "modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo\n",
    "modelo_entropy_pruned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predecir con el modelo Gini\n",
    "y_pred_gini = modelo_gini.predict(X_test_scaled)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a\n",
    "y_pred_entropy = modelo_entropy.predict(X_test_scaled)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a podado\n",
    "y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas para el modelo Gini (accuracy, precision, recall, f1-score)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a podado (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy_pruned = accuracy_score(y_test, y_pred_entropy_pruned)\n",
    "precision_entropy_pruned = precision_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "recall_entropy_pruned = recall_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "f1_entropy_pruned = f1_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "\n",
    "# Mostrar las m√©tricas\n",
    "print(\"M√©tricas del Modelo Gini:\")\n",
    "print(f\"Accuracy: {accuracy_gini:.4f}, Precision: {precision_gini:.4f}, Recall: {recall_gini:.4f}, F1-Score: {f1_gini:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a:\")\n",
    "print(f\"Accuracy: {accuracy_entropy:.4f}, Precision: {precision_entropy:.4f}, Recall: {recall_entropy:.4f}, F1-Score: {f1_entropy:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a Podado:\")\n",
    "print(f\"Accuracy: {accuracy_entropy_pruned:.4f}, Precision: {precision_entropy_pruned:.4f}, Recall: {recall_entropy_pruned:.4f}, F1-Score: {f1_entropy_pruned:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211cb88",
   "metadata": {},
   "source": [
    "## Conclusi√≥n:\n",
    "\n",
    "El balanceo no gener√≥ cambios en las m√©tricas, tanto con como sin normalizaci√≥n, ya que el conjunto de datos original ya estaba equilibrado entre clases. Por eso, el rendimiento del modelo se mantuvo pr√°cticamente igual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23e255",
   "metadata": {},
   "source": [
    "#### 7. √Årbol de Decisiones - CC:SI - ED:SI - Outliers:SI - Balanceo: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f175ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o original: (5200, 11)\n",
      "Tama√±o sin outliers: (5055, 11)\n",
      "M√©tricas del Modelo Gini:\n",
      "Accuracy: 0.7883, Precision: 0.8018, Recall: 0.7883, F1-Score: 0.7908\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a:\n",
      "Accuracy: 0.7943, Precision: 0.8060, Recall: 0.7943, F1-Score: 0.7945\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a Podado:\n",
      "Accuracy: 0.7943, Precision: 0.8060, Recall: 0.7943, F1-Score: 0.7945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datos a utilizar\n",
    "data_model_7 = data.copy()\n",
    "\n",
    "# ==========================\n",
    "# Eliminar outliers (IQR)\n",
    "# ==========================\n",
    "\n",
    "# seleccionar solo las columnas num√©ricas\n",
    "num_cols = data_model_7.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# calcular Q1, Q3 y el rango intercuart√≠lico (IQR)\n",
    "Q1 = data_model_7[num_cols].quantile(0.25)\n",
    "Q3 = data_model_7[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# crear una m√°scara booleana que identifique las filas SIN outliers\n",
    "mask = ~((data_model_7[num_cols] < (Q1 - 1.5 * IQR)) |\n",
    "         (data_model_7[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# filtrar los datos limpios\n",
    "data_clean = data_model_7[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o original:\", data_model_7.shape)\n",
    "print(\"Tama√±o sin outliers:\", data_clean.shape)\n",
    "\n",
    "# ==========================\n",
    "# Separar X e y\n",
    "# ==========================\n",
    "X = data_clean.drop(\"Workout_Type\", axis=1)\n",
    "y = data_clean[\"Workout_Type\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5200, stratify=y)\n",
    "\n",
    "# üîπ Escalado de las caracter√≠sticas (excepto 'Gender' y 'Workout_Type')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Seleccionar columnas num√©ricas que no sean categ√≥ricas\n",
    "numeric_cols = [col for col in X_train.columns if col not in ['Gender', 'Workout_Type']]\n",
    "\n",
    "# Hacer copias para no perder las columnas categ√≥ricas\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Escalar solo las columnas num√©ricas\n",
    "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Se crea un √°rbol con el criterio de impureza de Gini y una profundidad m√°xima de 5\n",
    "modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=5200)\n",
    "# Se entrena el modelo con los datos de entrenamiento\n",
    "modelo_gini.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Se crea un √°rbol con el criterio de Entrop√≠a\n",
    "modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=5200)\n",
    "# Se entrena el modelo\n",
    "modelo_entropy.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Se crea un √°rbol con Entrop√≠a y una condici√≥n de poda adicional\n",
    "modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=5200)\n",
    "# Se entrena el modelo\n",
    "modelo_entropy_pruned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predecir con el modelo Gini\n",
    "y_pred_gini = modelo_gini.predict(X_test_scaled)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a\n",
    "y_pred_entropy = modelo_entropy.predict(X_test_scaled)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a podado\n",
    "y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas para el modelo Gini (accuracy, precision, recall, f1-score)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a podado (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy_pruned = accuracy_score(y_test, y_pred_entropy_pruned)\n",
    "precision_entropy_pruned = precision_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "recall_entropy_pruned = recall_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "f1_entropy_pruned = f1_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "\n",
    "# Mostrar las m√©tricas\n",
    "print(\"M√©tricas del Modelo Gini:\")\n",
    "print(f\"Accuracy: {accuracy_gini:.4f}, Precision: {precision_gini:.4f}, Recall: {recall_gini:.4f}, F1-Score: {f1_gini:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a:\")\n",
    "print(f\"Accuracy: {accuracy_entropy:.4f}, Precision: {precision_entropy:.4f}, Recall: {recall_entropy:.4f}, F1-Score: {f1_entropy:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a Podado:\")\n",
    "print(f\"Accuracy: {accuracy_entropy_pruned:.4f}, Precision: {precision_entropy_pruned:.4f}, Recall: {recall_entropy_pruned:.4f}, F1-Score: {f1_entropy_pruned:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a70650",
   "metadata": {},
   "source": [
    "## Conclusi√≥n:\n",
    "\n",
    "La eliminaci√≥n de outliers mejor√≥ las m√©tricas del modelo, indicando que los datos at√≠picos afectaban su desempe√±o. Al limpiar el conjunto de datos, los √°rboles de decisi√≥n lograron una clasificaci√≥n m√°s precisa y estable. Sin embargo, las m√©tricas siguen siendo las mismas que cuando las varibles no estaban normalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc5a64",
   "metadata": {},
   "source": [
    "#### 8. √Årbol de Decisiones - CC:SI - ED:SI - Outliers:SI - Balanceo: SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e235d4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o original: (5200, 11)\n",
      "Tama√±o sin outliers: (5055, 11)\n",
      "M√©tricas del Modelo Gini:\n",
      "Accuracy: 0.7883, Precision: 0.8018, Recall: 0.7883, F1-Score: 0.7908\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a:\n",
      "Accuracy: 0.8002, Precision: 0.8114, Recall: 0.8002, F1-Score: 0.8018\n",
      "\n",
      "M√©tricas del Modelo Entrop√≠a Podado:\n",
      "Accuracy: 0.8002, Precision: 0.8114, Recall: 0.8002, F1-Score: 0.8018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datos a utilizar\n",
    "data_model_7 = data.copy()\n",
    "\n",
    "# ==========================\n",
    "# Eliminar outliers (IQR)\n",
    "# ==========================\n",
    "\n",
    "# seleccionar solo las columnas num√©ricas\n",
    "num_cols = data_model_7.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# calcular Q1, Q3 y el rango intercuart√≠lico (IQR)\n",
    "Q1 = data_model_7[num_cols].quantile(0.25)\n",
    "Q3 = data_model_7[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# crear una m√°scara booleana que identifique las filas SIN outliers\n",
    "mask = ~((data_model_7[num_cols] < (Q1 - 1.5 * IQR)) |\n",
    "         (data_model_7[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# filtrar los datos limpios\n",
    "data_clean = data_model_7[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o original:\", data_model_7.shape)\n",
    "print(\"Tama√±o sin outliers:\", data_clean.shape)\n",
    "\n",
    "# ==========================\n",
    "# Separar X e y\n",
    "# ==========================\n",
    "X = data_clean.drop(\"Workout_Type\", axis=1)\n",
    "y = data_clean[\"Workout_Type\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5200, stratify=y)\n",
    "\n",
    "# üîπ Escalado de las caracter√≠sticas (excepto 'Gender' y 'Workout_Type')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Seleccionar columnas num√©ricas que no sean categ√≥ricas\n",
    "numeric_cols = [col for col in X_train.columns if col not in ['Gender', 'Workout_Type']]\n",
    "\n",
    "# Hacer copias para no perder las columnas categ√≥ricas\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Escalar solo las columnas num√©ricas\n",
    "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Se crea un √°rbol con el criterio de impureza de Gini y una profundidad m√°xima de 5\n",
    "modelo_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo con los datos de entrenamiento\n",
    "modelo_gini.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Se crea un √°rbol con el criterio de Entrop√≠a\n",
    "modelo_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo\n",
    "modelo_entropy.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Se crea un √°rbol con Entrop√≠a y una condici√≥n de poda adicional\n",
    "modelo_entropy_pruned = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5, random_state=5200, class_weight='balanced')\n",
    "# Se entrena el modelo\n",
    "modelo_entropy_pruned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predecir con el modelo Gini\n",
    "y_pred_gini = modelo_gini.predict(X_test_scaled)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a\n",
    "y_pred_entropy = modelo_entropy.predict(X_test_scaled)\n",
    "\n",
    "# Predecir con el modelo de Entrop√≠a podado\n",
    "y_pred_entropy_pruned = modelo_entropy_pruned.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas para el modelo Gini (accuracy, precision, recall, f1-score)\n",
    "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
    "precision_gini = precision_score(y_test, y_pred_gini, average='weighted')\n",
    "recall_gini = recall_score(y_test, y_pred_gini, average='weighted')\n",
    "f1_gini = f1_score(y_test, y_pred_gini, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy, average='weighted')\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy, average='weighted')\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy, average='weighted')\n",
    "\n",
    "# M√©tricas para el modelo de Entrop√≠a podado (accuracy, precision, recall, f1-score)\n",
    "accuracy_entropy_pruned = accuracy_score(y_test, y_pred_entropy_pruned)\n",
    "precision_entropy_pruned = precision_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "recall_entropy_pruned = recall_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "f1_entropy_pruned = f1_score(y_test, y_pred_entropy_pruned, average='weighted')\n",
    "\n",
    "# Mostrar las m√©tricas\n",
    "print(\"M√©tricas del Modelo Gini:\")\n",
    "print(f\"Accuracy: {accuracy_gini:.4f}, Precision: {precision_gini:.4f}, Recall: {recall_gini:.4f}, F1-Score: {f1_gini:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a:\")\n",
    "print(f\"Accuracy: {accuracy_entropy:.4f}, Precision: {precision_entropy:.4f}, Recall: {recall_entropy:.4f}, F1-Score: {f1_entropy:.4f}\\n\")\n",
    "print(\"M√©tricas del Modelo Entrop√≠a Podado:\")\n",
    "print(f\"Accuracy: {accuracy_entropy_pruned:.4f}, Precision: {precision_entropy_pruned:.4f}, Recall: {recall_entropy_pruned:.4f}, F1-Score: {f1_entropy_pruned:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae62506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO guardar m√©tricas en el diccionario\n",
    "# TODO hacer la importancia de variables y gr√°ficar el arbol gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3bba53",
   "metadata": {},
   "source": [
    "## Conclusi√≥n:\n",
    "\n",
    "Con el balanceo de clases y la eliminaci√≥n de outliers, el modelo mantuvo m√©tricas similares a las obtenidas sin escalar las variables. Aunque el modelo con entrop√≠a logr√≥ un buen desempe√±o (accuracy = 0.8002, precisi√≥n = 0.8114, recall = 0.8002 y F1-score = 0.8018), el escalado no tuvo impacto, ya que los √°rboles de decisi√≥n no dependen de la escala de las variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
